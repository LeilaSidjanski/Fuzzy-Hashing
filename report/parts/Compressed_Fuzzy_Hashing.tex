\newpage
\include{Fuzzy_Hashing.tex}

\section{Compressed Fuzzy Hashing}
\label{sec:Compressed Fuzzy Hashing}

Compressed fuzzy hashing generates concise hashes of data, capturing key features while allowing for slight variations - a concept referred to as "fuzziness". Unlike conventional cryptographic hashing, which generates fixed-length outputs and is highly sensitive to even minimal changes in input, compressed fuzzy hashing maintains a robust link to the original content despite alterations.
This section delves into the mechanics of the compressed fuzzy hashing algorithm, called the postHash procedure. We will explore the mathematical concepts that enable its effectiveness and discuss experimental results that demonstrate its capabilities and performance on our dataset.

\subsection{PostHashing Algorithm}

The postHash algorithm, constituting the second and final step in the fuzzy hashing process, generates compact hashes based on the indices derived from the preHash algorithm (see Section \ref{sec:Fuzzy Hashing}). This algorithm is designed to transform a tuple of indices into a concise hash format, \(h_1 || \ldots || h_m\), where each \(h_i\), for \(i \in [1, m]\), is an integer within the range \([0, \ldots, D-1]\).The parameter \(D\), defined as \(2^d\), denotes the total number of possible values each \(h_i\) can assume, with \(d\) representing the bit depth used per index. 

Algorithm Inputs and Outputs:
\begin{enumerate}
    \item \textbf{Inputs}: As illustrated in Figure~\ref{postHash Algorithm}, the algorithm takes as input:
    \begin{itemize}
        \item \textbf{A tuple of indices}: \((i_1, \ldots, i_m)\), which is the output of the preHash algorithm
    \end{itemize}
    \item \textbf{Output}: Converted indices, forming a compact hash \(h_1|| \ldots || h_m\), where \(h_i\) \(i \in [1, m]\), is an integer in \([0, \ldots, D-1]\)
\end{enumerate}

Detailed process of postHash, for the case when $d < 4$:
\begin{enumerate}
    \item \textbf{Subroutine}: For each index provided by the preHash, subroutine \(T\) checks if the index is within the acceptable range. If an index is out of range, \(T\) returns 0, otherwise, it proceeds with a table lookup. The subroutine maps each valid index \(i\) to an integer nearly uniformly distributed between \([0, D-1]\), ensuring that all potential hash values are equally probable. This mapping is important for the security and effectiveness of the fuzzy hashing system.
    
    The subroutine function, \(T\), works as follows:
    \begin{itemize}
        \item \textbf{Inputs}: 
        \begin{itemize}
            \item \textbf{Shifted Index}: Each index from the preHash output tuple \((i_1, \ldots, i_m)\) is adjusted before conversion into a hash component. Specifically, the index \(i\) is shifted by subtracting a previously determined index \(i'\), resulting in a transformed index \(i - i'\). This shifting process normalizes the indices, maintaining a consistent relative positioning within the dataset, which helps in achieving a more uniform distribution of hash values across the range \([0, D-1]\).
            \item \textbf{d}: Represents the bit length used for each hash index \(h_i\). This value determines \(D = 2^d\), the total number of distinct values each \(h_i\) can take, effectively setting the hash resolution.
        \end{itemize}
        \item \textbf{Output}: \(h_i\), the mapped integer
    \end{itemize}
\end{enumerate}

For the case when $d = 4$, a different subroutine called \textit{scambled\_T} is used. This subroutine corresponds to a partition problem, which is detailed at the end of Section~\ref{sec:Experimental_derivation_q}. Although the mapping of integers differs in this subroutine, it similarly takes an index from the preHash tuple as input and maps it to an integer, just as in the process for $d < 4$.

\vspace{20pt}

\begin{algorithm}
    \begin{algorithmic}[1]
    \caption{postHash Algorithm}
    \label{postHash Algorithm}
    \Function{(postHash $\circ$ preHash$_\text{key}^m$)}{$X$}
    \State $\text{hash} = []$
    \State $i' \gets 0$
    \For{$i \in \text{indices}$}
        \State $h_i \gets \text{Subroutine}(i - i', d)$
        \State $\text{hash.append}(h_i)$
        \State $i' \gets i$
    \EndFor 
    \State \Return{$h_1, \ldots, h_m$}
    \EndFunction
    \end{algorithmic}
    \end{algorithm}
    
    \begin{algorithm}
    \begin{algorithmic}[1]
    \caption{\textit{Subroutine} Algorithm}
    \label{Subroutine Algorithm}
    \Function{\text{Subroutine}}{$i$, $d$}
    \State $p = 0.03514$
    \State $h_i = \lfloor 2^d (1-p)^{i} \rfloor$
    \State \Return{$h_i$}
    \EndFunction
    \end{algorithmic}
    \end{algorithm}

\subsection{Assessing Similarity of Biometric Inputs After PostHash Application}
\label{sec:q}

After processing finger images through the pipeline referenced in Pipeline~\ref{pipeline_simon} to extract their feature vectors, the resulting data undergo the preHash and subsequently, the postHash algorithms. The final output from postHash consists of a set of integers \( h_i \), each bounded within the inclusive range \([0, D-1]\). This process effectively assigns each feature vector index to an integer within this range.

In our methodology, we model these integers \( h_i \) as following a geometric distribution. This assumption is based on the output relationship established by \( Hash_{key}^m(X) = postHash(preHash_{key}^m(X)) \). We assume the same conditions as discussed in Section~\ref{sec:Fuzzy Hashing}: the key is randomly chosen, and \(k\) represents a uniformly distributed random index. Consequently, the probability of the Hash operation yielding the same index for two different inputs \(X\) and \(Y\) can be mathematically characterized as follows:

\[Pr[Hash_{key}^m(d_X) = Hash_{key}^m(d_Y)] \leq \mu^m(1 - \frac{1}{D}) + \frac{1}{D}\]

where equality is reached for the optimal offset translations.

The overall probability \( q \) of a matching hash index under random inputs \( X \) and \( Y \), reflecting their distribution, is denoted by:

\begin{equation}
    q = \mu^m\left(1 - \frac{1}{D}\right) + \frac{1}{D}
    \label{eq:q_formula}
\end{equation}
    

Based on the range of possible values for \(d\), we have computed several specific instances of \(q\) (theoretical values given by the above equation):
\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.25}\begin{tabular}{|c|c|c|c|}
        \hline
        & $q_{same}$ & $q_{diff}$ & $q_{indep}$\\
        \hline
        \(m = 1\) \(d = 1\) & $61.03\%$ & $54.12\%$ & $50.73\%$\\
        \(m = 1\) \(d = 2\) & $41.54\%$ & $31.19\%\%$ & $26.09\%$\\
        \(m = 1\) \(d = 3\) & $31.79\%$ & $19.72\%$ & $13.77\%$\\
        \(m = 1\) \(d = 4\) & $26.92\%$ & $13.98\%$ & $7.61\%$\\
        \hline
    \end{tabular}
\caption{Comparison of Distributions: $q_{same}$, $q_{diff}$, and $q_{indep}$}
\label{tab:theoretical_q}
\end{table}

\subsection{Experimental Derivation of the probabilities $q_{same}^{obs}$, $q_{diff}^{obs}$, and $q_{indep}^{obs}$ for different \(m\), \(d\) parameter combinations} \label{sec:Experimental_derivation_q}

In the subsequent section, we explore enhancements to data compression techniques within our fuzzy hashing framework, emphasizing the use of various \(m\) and \(d\) parameter combinations. We initiate our analysis by conducting experiments to determine \(q_{same}^{obs}\), \(q_{diff}^{obs}\), and \(q_{indep}^{obs}\).

Building on the foundational calculations for \(q_{\text{indep}}\), which we derived using Formula ~\ref{eq:q_formula} with \( \mu^{obs}_{\text{indep}} \) previously identified in Section \ref{sec:Fuzzy Hashing}, our exploration extended to \(q^{obs}_{\text{same}}\) and \(q^{obs}_\text{diff}\). To evaluate these parameters, we initiated a series of experiments. Integral to this implementation and the following ones, is the creation of the lookup table, to convert indices to their compressed counterparts. The generation of this table and all subsequent ones, is governed by a geometrically-derived formula that accurately encapsulates the transformation of index values. The specific formula is presented as:

\begin{equation}
    \text{postHash}(i) = \left\lfloor 2^d \left(1 - p\right)^i \right\rfloor
    \label{eq:GeomFormula}
\end{equation}
    
 
Leveraging the above formula, we have developed lookup tables that convert a single index (\(m = 1\)) into various bit representations (\(d=1\), \(d=2\), \(d=3\), and \(d=4\)). Each entry in these tables represents an integer, denoted by \(i\), and includes a corresponding hash output and the probability of that output occurring.

Binary Hash Output Distribution Table (\(d=1\))
{\renewcommand{\arraystretch}{1.25}
\[
\text{postHash}(i) = \left\{
    \begin{array}{lll}
        \text{1}  & \text{if } 1 \leq i \leq 19 & (\text{Pr} = 49.32\%)
        \\
        \text{0}  & \text{if } 20 \leq i & (\text{Pr} = 50.68\%)
        \\
    \end{array}
\right.    
\]}


Quaternary Hash Output Distribution Table (\(d=2\)):
{\renewcommand{\arraystretch}{1.25}
\[
\text{PostHash}(i) = \left\{
    \begin{array}{lll}
        \text{3}  & \text{if } 1 \leq i \leq 8 & (\text{Pr} = 24.89\%)
        \\
        \text{2}  & \text{if } 9 \leq i \leq 19 & (\text{Pr} = 24.43\%)
        \\
        \text{1}  & \text{if } 20 \leq i \leq 38 & (\text{Pr} = 25\%)
        \\
        \text{0}  & \text{if } 39 \leq i & (\text{Pr} = 25.68\%)
        \\
    \end{array}
\right.    
\]}

Octal Hash Output Distribution Table (\(d=3\)):
{\renewcommand{\arraystretch}{1.25}
\[
\text{postHash}(i) = \left\{
    \begin{array}{lll}
        \text{7}  & \text{if } 1 \leq i \leq 3 & (\text{Pr} = 10.18\%)
        \\
        \text{6}  & \text{if } 4 \leq i \leq 8 & (\text{Pr} = 14.71\%)
        \\
        \text{5}  & \text{if } 9 \leq i \leq 13 & (\text{Pr} = 12.3\%)
        \\
        \text{4}  & \text{if } 14 \leq i \leq 19 & (\text{Pr} = 12.13\%)
        \\
        \text{3}  & \text{if } 20 \leq i \leq 27 & (\text{Pr} = 12.61\%)
        \\
        \text{2}  & \text{if } 28 \leq i \leq 38 & (\text{Pr} = 12.38\%)
        \\
        \text{1}  & \text{if } 39 \leq i \leq 58 & (\text{Pr} = 13.12\%)
        \\
        \text{0}  & \text{if } 59 \leq i & (\text{Pr} = 12.59\%)
        \\
    \end{array}
\right.    
\]}


Hexadecimal Hash Output Distribution Table \(d=4\)
{
\renewcommand{\arraystretch}{1.25}
\[
\text{postHash}(i) = \left\{
\begin{array}{lll}
    \text{15} & \text{if } i = 1 & (\text{Pr} = 3.51\%), \\
    \text{14} & \text{if } 2 \leq i \leq 3 & (\text{Pr} = 6.66\%) \\
    \text{13} & \text{if } 4 \leq i \leq 5 & (\text{Pr} = 6.2\%) \\
    \text{12} & \text{if } 6 \leq i \leq 8 & (\text{Pr} = 8.51\%) 
    \\
    \text{11} & \text{if } 9 \leq i \leq 10 & (\text{Pr} = 5.19\%) \\
    \text{10} & \text{if } 11 \leq i \leq 13 & (\text{Pr} = 7.12\%) \\
    \text{9} & \text{if } 14 \leq i \leq 16 & (\text{Pr} = 6.39\%) \\
    \text{8} & \text{if } 17 \leq i \leq 19 & (\text{Pr} = 5.74\%) \\
    \text{7} & \text{if } 20 \leq i \leq 23 & (\text{Pr} = 6.76\%) \\
    \text{6} & \text{if } 24 \leq i \leq 27 & (\text{Pr} = 5.86\%) \\
    \text{5} & \text{if } 28 \leq i \leq 32 & (\text{Pr} = 6.23\%) \\
    \text{4} & \text{if } 33 \leq i \leq 38 & (\text{Pr} = 6.15\%) \\
    \text{3} & \text{if } 39 \leq i \leq 46 & (\text{Pr} = 6.39\%) \\
    \text{2} & \text{if } 47 \leq i \leq 58 & (\text{Pr} = 6.73\%) \\
    \text{1} & \text{if } 59 \leq i \leq 77 & (\text{Pr} = 6.19\%) \\
    \text{0} & \text{if } 78 \leq i \leq n & (\text{Pr} = 6.36\%)
\end{array}
\right.
\]
}

In our current implementation of postHash output tables, each partition does not distribute probabilities equally among the bins. This uneven distribution, observed from binary to hexadecimal outputs, poses some challenges, particularly in our system, where hash functions are employed to ensure the confidentiality of biometric data. The non-uniform distribution can lead to predictable patterns, potentially compromising the security by making the system more vulnerable to attacks such as hash collisions. These predictable patterns can be exploited by attackers to reverse-engineer or guess hash values, thereby breaching the confidentiality of biometric information and undermining the integrity of our security measures.

In the context of compressed fuzzy hashing, domain scrambling aims to reorder the indices in a way that the resulting hash values are "nearly" uniformly distributed. This task shares similarities with a well known problem in computer science called the "Partition Problem". This problem involves deciding whether a given multiset of positive integers can be partitioned into two or more subsets such that the sum of the numbers in each subset is equal. This problem is a specific instance of the subset sum problem, where the target sum is half of the total sum of all elements in the set. The partition problem is classified as NP-complete, a categorization in computational complexity theory used to describe decision problems. A problem is in NP (Non-deterministic Polynomial time) if a solution for the problem can be verified in polynomial time given a candidate solution. moreover, a problem is NP-complete if it is as hard as the hardest problems in NP and if every problem in NP can be reduced to it in polynomial time. This implies two things for NP-complete problems:

\begin{enumerate}
    \item Verification is Feasible: Given a ``solution,'' we can verify whether it is correct in polynomial time.
    \item There is no known algorithm that can find an optimal solution to NP-complete problems in polynomial time for all general cases.
\end{enumerate}

Given the NP-completeness of the partition problem, programming an optimal solution for domain scrambling — which is akin to a continuous partitioning challenge — is impractical. Theoretically, while it is feasible to verify if a given distribution of indices across hash buckets is optimal, finding that distribution algorithmically cannot be guaranteed to be efficient for all possible cases. However, we did try to implement this algorithm and observe the results for the case where the hash depth is \(d=4\), resulting in \(D=16\) possible hash values. The program written is designed to distribute a sequence of integers (1 to 90'240) across predefined buckets (0 to 15) to achieve a target probability distribution based on our geometric decay model~\ref{eq:GeomFormula}. This process begins by mapping each integer to a bucket using a calculated index derived from the geometric formula, adjusted by a scaling factor. Initial bucket assignments aim to approximate a uniform distribution of probabilities. The program then iteratively adjusts these assignments: integers are moved between buckets to better align with the target probabilities, ensuring that the sum of probabilities within each bucket remains close to the desired uniform distribution. This adjustment process continues until the distribution of probabilities across all buckets falls within an acceptable error margin, or until no further beneficial adjustments can be made. The following lookup table illustrates the distribution of indices across each bucket, including the count of indices falling within each bucket and the range of indices encompassed by it.



\renewcommand{\arraystretch}{1.25}
\[
\text{Bucket Assignments} = \left\{
\begin{array}{lll}
    0 & \text{if } i \in [1, 90240] & (\text{Count} = 82493, \text{Pr} = 6.297\%), \\
    1 & \text{if } i \in [2, 7750] & (\text{Count} = 1941, \text{Pr} = 6.276\%), \\
    2 & \text{if } i \in [3, 5812] & (\text{Count} = 1136, \text{Pr} = 6.264\%), \\
    3 & \text{if } i \in [4, 4679] & (\text{Count} = 807, \text{Pr} = 6.204\%), \\
    4 & \text{if } i \in [5, 3875] & (\text{Count} = 628, \text{Pr} = 6.294\%), \\
    5 & \text{if } i \in [6, 3251] & (\text{Count} = 514, \text{Pr} = 6.284\%), \\
    6 & \text{if } i \in [7, 2741] & (\text{Count} = 435, \text{Pr} = 6.237\%), \\
    7 & \text{if } i \in [8, 2310] & (\text{Count} = 378, \text{Pr} = 6.295\%), \\
    8 & \text{if } i \in [9, 1936] & (\text{Count} = 334, \text{Pr} = 6.298\%), \\
    9 & \text{if } i \in [10, 1608] & (\text{Count} = 301, \text{Pr} = 6.292\%), \\
    10 & \text{if } i \in [11, 1313] & (\text{Count} = 272, \text{Pr} = 6.214\%), \\
    11 & \text{if } i \in [12, 1047] & (\text{Count} = 250, \text{Pr} = 6.293\%), \\
    12 & \text{if } i \in [13, 804] & (\text{Count} = 232, \text{Pr} = 6.298\%), \\
    13 & \text{if } i \in [14, 580] & (\text{Count} = 215, \text{Pr} = 6.127\%), \\
    14 & \text{if } i \in [15, 373] & (\text{Count} = 201, \text{Pr} = 6.061\%), \\
    15 & \text{if } i \in [76, 180] & (\text{Count} = 103, \text{Pr} = 6.267\%)
\end{array}
\right.
\]


We can see that the distribution of probabilities and counts across buckets demonstrates a good effort towards achieving uniformity.

Utilizing these derived lookup tables, alongside our established preHash function(~\ref{preHash Algorithm}) and the newly implemented postHash function(~\ref{postHash Algorithm}), we are equipped to conduct experiments on our dataset to assess \(q_{same}\) and \(q_{diff}\) for the \(4\) different combinations of the parameters \(d\) and \(m\). The modification in our pipeline involves passing the output from the preHash function directly into the postHash function. Subsequently, we calculate the Hamming distance between each image pair at the conclusion of both the preHash and postHash stages. This method allows us to measure the similarity of the biometric inputs after the complete application of our hashing process.

\begin{enumerate}
    \item \text{\(d = 1\) and \(m = 1\)}: Single-index outputs (\(m=1\)) and binary postHash indices (\(d=1\)).
    \begin{itemize}
        \item \(q_{same}^{obs} = 61.53\%\)
        \item \(q_{diff}^{obs} = 52.69\%\)
    \end{itemize}

    \begin{figure}[H]
        \centering
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/d1same.png}
            \caption{Count of the number of matches for Same, Aligned Biometric Samples with Single Index Hashing and Binary PostHash Indices}
            \label{mu_same}
        \end{minipage}
        \hfill
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/d1diff.png}
            \caption{Count of the number of matches for Different, Aligned Biometric Samples with Single Index Hashing and Binary PostHash Indices}
            \label{mu_diff}
        \end{minipage}
    \end{figure}
    
    \item \text{\(d = 2\) and \(m = 1\)}: Single-index outputs (\(m=1\)) and quaternary postHash indices (\(d=2\)).
    \begin{itemize}
        \item \(q_{same}^{obs} = 42.62\%\)
        \item \(q_{diff}^{obs} = 29.84\%\)
    \end{itemize}

    \begin{figure}[H]
        \centering
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/d2same.png}
            \caption{Count of the number of matches for Same, Aligned Biometric Samples with Single Index Hashing and Quaternary PostHash Indices}
            \label{mu_same}
        \end{minipage}
        \hfill
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/d2diff.png}
            \caption{Count of the number of matches for Different, Aligned Biometric Samples with Single Index Hashing and Quaternary PostHash Indices}
            \label{mu_diff}
        \end{minipage}
    \end{figure}
    
    \item \text{\(d = 3\) and \(m = 1\)}: Single-index outputs (\(m=1\)) and octal postHash indices (\(d=3\)).
    \begin{itemize}
        \item \(q_{same}^{obs} = 31.97\%\)
        \item \(q_{diff}^{obs} = 17.36\%\)
    \end{itemize}

    \begin{figure}[H]
        \centering
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/d3same.png}
            \caption{Count of the number of matches for Same, Aligned Biometric Samples with Single Index Hashing and Octal PostHash Indices}
            \label{mu_same}
        \end{minipage}
        \hfill
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/d3diff.png}
            \caption{Count of the number of matches for Different, Aligned Biometric Samples with Single Index Hashing and Octal PostHash Indices}
            \label{mu_diff}
        \end{minipage}
    \end{figure}
    
    \item \text{\(d = 4\) and \(m = 1\)}: Single-index outputs (\(m=1\)) and hexa-decimal postHash indices (\(d=4\)).
    \begin{itemize}
        \item \(q_{same}^{obs} = 26.92\%\)
        \item \(q_{diff}^{obs} = 13.13\%\)
    \end{itemize}

    \begin{figure}[H]
        \centering
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/d4same.png}
            \caption{Count of the number of matches for Same, Aligned Biometric Samples with Single Index Hashing and Hexa-Decimal PostHash Indices}
            \label{mu_same}
        \end{minipage}
        \hfill
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/d4diff.png}
            \caption{Count of the number of matches for Different, Aligned Biometric Samples with Single Index Hashing and Hexa-Decimal PostHash Indices}
            \label{mu_diff}
        \end{minipage}
    \end{figure}
\end{enumerate}

The theoretical values for \(q_{same}\) presented in Table~\ref{tab:theoretical_q} are accurate approximations, as the experimental results closely match these values. The greater discrepancy between theoretical and experimental values for \(q_{diff}\) compared to \(q_{same}\) can be attributed to the following factor: Different biometric samples exhibit higher variability, leading to a broader range of feature differences, which can cause deviations from theoretical assumptions. Outliers and noise further exacerbate the discrepancies for \(q_{diff}\). Moreover, the theoretical values for the parameter combination \(m=1\) and \(d=4\) (last row of Table~\ref{tab:theoretical_q}) align almost perfectly with the experimental results, with \(q_{same}\) values being identical and \(q_{diff}\) values differing by only approximately 0.8\%. The lookup table used for this experiment was generated using Equation~\ref{eq:GeomFormula}, followed by a redistribution of the indices across each bucket to achieve a more uniform distribution. This redistribution process was not applied to the first three experiments, which could explain why their results show greater discrepancies.


