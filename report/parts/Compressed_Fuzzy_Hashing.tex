\newpage
\include{Fuzzy_Hashing.tex}

\section{Compressed Fuzzy Hashing}
\label{sec:Compressed Fuzzy Hashing}

Compressed fuzzy hashing generates concise hashes of data, capturing key features while allowing for slight variations - a concept referred to as "fuzziness". Unlike conventional cryptographic hashing, which generates fixed-length outputs and is highly sensitive to even minimal changes in input, compressed fuzzy hashing maintains a robust link to the original content despite alterations.
This section delves into the mechanics of the compressed fuzzy hashing algorithm, called the \textit{PostHash} procedure. We will explore the mathematical concepts that enable its effectiveness and discuss experimental results that demonstrate its capabilities and performance on our dataset.

\subsection{PostHashing Algorithm}

The \textit{PostHash} algorithm, constituting the second and final step in the fuzzy hashing process, generates compact hashes based on the indices derived from the \textit{PreHash} algorithm (see section \ref{sec:Fuzzy Hashing}). This algorithm is designed to transform a tuple of indices into a concise hash format, \(h_1 || \ldots || h_m\), where each \(h_i\), for \(i \in [1, m]\), is an integer within the range \([0, \ldots, D-1]\).The parameter \(D\), defined as \(2^d\), denotes the total number of possible values each \(h_i\) can assume, with \(d\) representing the bit depth used per index. 

Algorithm Inputs and Outputs:
\begin{enumerate}
    \item \textbf{Inputs}: As illustrated in Figure~\ref{postHash Algorithm}, the algorithm takes as input:
    \begin{itemize}
        \item \textbf{A tuple of indices}: \((i_1, \ldots, i_m)\), which is the output of the \textit{PreHash} algorithm
    \end{itemize}
    \item \textbf{Output}: Converted indices, forming a compact hash \(h_1|| \ldots || h_m\), where \(h_i\) \(i \in [1, m]\), is an integer in \([0, \ldots, D-1]\)
\end{enumerate}

Detailed Process of \textit{PostHash}:
\begin{enumerate}
    \item \textbf{Subroutine}: For each index provided by the \textit{PreHash}, subroutine \(T\) checks if the index is within the acceptable range. If an index is out of range, \(T\) returns 0, otherwise, it proceeds with a table lookup. The subroutine maps each valid index \(i\) to an integer nearly uniformly distributed between \([0, D-1]\), ensuring that all potential hash values are equally probable. This mapping is important for the security and effectiveness of the fuzzy hashing system.
    
    The subroutine function, \(T\), works as follows:
    \begin{itemize}
        \item \textbf{Inputs}: 
        \begin{itemize}
            \item \textbf{Shifted Index}: Each index from the \textit{PreHash} output tuple \((i_1, \ldots, i_m)\) is adjusted before conversion into a hash component. Specifically, the index \(i\) is shifted by subtracting a previously determined index \(i'\), resulting in a transformed index \(i - i'\). This shifting process normalizes the indices, maintaining a consistent relative positioning within the dataset, which helps in achieving a more uniform distribution of hash values across the range \([0, D-1]\).
            \item \textbf{d}: Represents the bit length used for each hash index \(h_i\). This value determines \(D = 2^d\), the total number of distinct values each \(h_i\) can take, effectively setting the hash resolution.
        \end{itemize}
        \item \textbf{Output}: \(h_i\), the mapped integer
    \end{itemize}
\end{enumerate}

\vspace{20pt}

\begin{algorithm}
    \begin{algorithmic}[1]
    \caption{\textit{postHash} Algorithm}
    \label{postHash Algorithm}
    \Function{(postHash $\circ$ preHash$_\text{key}^m$)}{$X$}
    \State $\text{hash} = []$
    \State $i' \gets 0$
    \For{$i \in \text{indices}$}
        \State $h_i \gets \text{Subroutine}(i - i', d)$
        \State $\text{hash.append}(h_i)$
        \State $i' \gets i$
    \EndFor 
    \State \Return{$h_1, \ldots, h_m$}
    \EndFunction
    \end{algorithmic}
    \end{algorithm}
    
    \begin{algorithm}
    \begin{algorithmic}[1]
    \caption{\textit{Subroutine} Algorithm}
    \label{Subroutine Algorithm}
    \Function{\text{Subroutine}}{$i$, $d$}
    \State $p = 0.0329$
    \State $h_i = \lfloor 2^d (1-p)^{i} \rfloor$
    \State \Return{$h_i$}
    \EndFunction
    \end{algorithmic}
    \end{algorithm}

\subsection{Assessing Similarity of Biometric Inputs After PostHash Application}
\label{sec:q}

After processing finger images through the pipeline referenced in Pipeline~\ref{pipeline_simon} to extract their feature vectors, the resulting data undergo the \textit{preHash} and subsequently, the \textit{postHash} algorithms. The final output from \textit{postHash} consists of a set of integers \( h_i \), each bounded within the inclusive range \([0, D-1]\). This process effectively assigns each feature vector index to an integer within this range.

In our methodology, we model these integers \( h_i \) as following a geometric distribution. This assumption is based on the output relationship established by \( Hash_{key}^m(X) = \text{postHash}(\text{preHash}_{key}^m(X)) \). We assume the same conditions as discussed in Section~\ref{sec:Fuzzy Hashing}: the key is randomly chosen, and \(k\) represents a uniformly distributed random index. Consequently, the probability of the \textit{Hash} operation yielding the same index for two different inputs \(X\) and \(Y\) can be mathematically characterized as follows:

\[Pr[Hash_{key}^m(d_X) = Hash_{key}^m(d_Y)] \leq \mu^m(1 - \frac{1}{D}) + \frac{1}{D}\]

where equality is reached for the optimal offset translations.

The overall probability \( q \) of a matching hash index under random inputs \( X \) and \( Y \), reflecting their distribution, is denoted by:

\[
q = \mu^m(1 - \frac{1}{D}) + \frac{1}{D}
\]

Based on the range of possible values for \(d\), we have computed several specific instances of \(q\):
\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.25}\begin{tabular}{|c|c|c|c|}
        \hline
        & $q_{same}$ & $q_{diff}$ & $q_{indep}$\\
        \hline
        m = 1 d = 1 & $60\%$ & $54\%$ & $51\%$\\
        m = 1 d = 2 & $40\%$ & $31\%$ & $26\%$\\
        m = 1 d = 3 & $30\%$ & $20\%$ & $14\%$\\
        m = 1 d = 4 & $25\%$ & $14\%$ & $8\%$\\
        \hline
    \end{tabular}
\caption{Comparison of Distributions: $q_{same}$, $q_{diff}$, and $q_{indep}$}
\end{table}

\subsection{Data Compression Techniques for m=1, d=4}

In this subsection, we focus on optimizing data compression strategies within our fuzzy hashing framework, specifically utilizing settings where \(m=1\) and \(d=4\). This configuration means that we use a single index (\(m=1\)) from our hashed data and encode each integer in our hash using 4 bits (\(d=4\)).

Integral to this implementation is the creation of the lookup table, to convert indices to their compressed counterparts. The generation of this table is governed by a geometrically-derived formula that accurately encapsulates the transformation of index values. The specific formula is presented as:

\[ 
\text{postHash}(i) = \left\lfloor 2^d \left(1 - p\right)^i \right\rfloor 
\]

Leveraging the above formula, we present the constructed lookup table for a four-bit representation, \(d=4\). Each entry in the table corresponds to an integer value, denoted by \(i\), and is associated with a specific hash output along with its calculated probability of occurrence.

{
\renewcommand{\arraystretch}{1.25}
\[
\text{postHash}(i) = \left\{
\begin{array}{lll}
    \text{15} & \text{if } i = 1 & (\text{Pr} = 3.29\%), \\
    \text{14} & \text{if } 2 \leq i \leq 3 & (\text{Pr} = 6.26\%), \\
    \text{13} & \text{if } 4 \leq i \leq 6 & (\text{Pr} = 8.64\%), \\
    \text{12} & \text{if } 7 \leq i \leq 8 & (\text{Pr} = 5.3\%), \\
    \text{11} & \text{if } 9 \leq i \leq 11 & (\text{Pr} = 7.31\%), \\
    \text{10} & \text{if } 12 \leq i \leq 14 & (\text{Pr} = 6.61\%), \\
    \text{9} & \text{if } 15 \leq i \leq 17 & (\text{Pr} = 6\%), \\
    \text{8} & \text{if } 18 \leq i \leq 20 & (\text{Pr} = 5.41\%), \\
    \text{7} & \text{if } 21 \leq i \leq 24 & (\text{Pr} = 6.41\%), \\
    \text{6} & \text{if } 25 \leq i \leq 29 & (\text{Pr} = 6.9\%), \\
    \text{5} & \text{if } 30 \leq i \leq 34 & (\text{Pr} = 5.83\%), \\
    \text{4} & \text{if } 35 \leq i \leq 41 & (\text{Pr} = 6.7\%), \\
    \text{3} & \text{if } 42 \leq i \leq 50 & (\text{Pr} = 6.6\%), \\
    \text{2} & \text{if } 51 \leq i \leq 62 & (\text{Pr} = 6.2\%), \\
    \text{1} & \text{if } 63 \leq i \leq 82 & (\text{Pr} = 6.13\%), \\
    \text{0} & \text{if } 83 \leq i \leq n & (\text{Pr} = 6.43\%),
\end{array}
\right.
\]
}

To enhance the equilibrium of occurrence probabilities associated with each integer in our hashing output, we employed a domain scrambling strategy. This method involves a deliberate, manual reassignment of indices to different categories. This careful reallocation ensures a more uniform probability distribution for each hashed integer, optimizing the overall balance of our compression scheme. 


\renewcommand{\arraystretch}{1.25}{
\[
\text{postHash}(i) = \left\{
\begin{array}{lll}
    15 & \text{if } i \in \{1\} \cup \{4\} & (\text{Pr} = 6.27\%), \\
    14 & \text{if } i \in \{2, 3\} & (\text{Pr} = 6.26\%), \\
    13 & \text{if } i \in \makecell[tl]{\{5, 6\} \cup \{53\}} & (\text{Pr} = 6.24\%), \\
    12 & \text{if } i \in \makecell[tl]{\{7, 8\} \cup \{38\}} & (\text{Pr} = 6.25\%), \\
    11 & \text{if } i \in \makecell[tl]{\{9, 10\} \cup \{29\}} & (\text{Pr} = 6.25\%), \\
    10 & \text{if } i \in \makecell[tl]{\{12, 13\} \cup \{83\} \cup \{85, \ldots, 92\} \\ \cup \{99\} \cup \{170\}} & (\text{Pr} = 6.24\%), \\
    9  & \text{if } i \in \makecell[tl]{\{15, \ldots, 17\} \cup \{78\}} & (\text{Pr} = 6.25\%), \\
    8  & \text{if } i \in \makecell[tl]{\{18, \ldots, 20\} \cup \{42\}} & (\text{Pr} = 6.24\%), \\
    7  & \text{if } i \in \makecell[tl]{\{21, \ldots, 23\} \cup \{63, 64\} \cup \{71\}} & (\text{Pr} = 6.25\%), \\
    6  & \text{if } i \in \makecell[tl]{\{25, \ldots, 28\} \cup \{61\} \cup \{84\}} & (\text{Pr} = 6.25\%), \\
    5  & \text{if } i \in \makecell[tl]{\{30, \ldots, 34\} \cup \{62\}} & (\text{Pr} = 6.26\%), \\
    4  & \text{if } i \in \makecell[tl]{\{35, \ldots, 37\} \cup \{39, \ldots, 41\} \cup \{57\}} & (\text{Pr} = 6.26\%), \\
    3  & \text{if } i \in \makecell[tl]{\{43, \ldots, 50\} \cup \{59\}} & (\text{Pr} = 6.24\%), \\
    2  & \text{if } i \in \makecell[tl]{\{11\} \cup \{51, 52\} \cup \{54, \ldots, 56\} \\ \cup \{58\} \cup \{60\} \cup \{98\}} & (\text{Pr} = 6.25\%), \\
    1  & \text{if } i \in \makecell[tl]{\{24\} \cup \{65, \ldots, 70\} \cup \{72, \ldots, 77\} \\ \cup \{79, \ldots, 82\}} & (\text{Pr} = 6.25\%), \\
    0  & \text{if } i \in \makecell[tl]{\{14\} \cup \{93, \ldots, 97\} \cup \{101\} \\ \cup \{103, \ldots, 169\} \cup \{171, \ldots, n\}} & (\text{Pr} = 6.24\%)
\end{array}
\right.
\]
}




\subsection{Efficiency Improvement in Hashing Process}