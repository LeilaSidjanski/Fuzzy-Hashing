\newpage
\include{Application:Private_Compact_Biometric_Matching.tex}
\section{Conclusion}
In this report, we have explored the application of fuzzy hashing to finger-vein biometric authentication, aiming to enhance both security and efficiency. Our investigation included the development and assessment of both the preHash and postHash algorithms, which transform biometric data into secure hash values that maintain consistency despite slight variations in the input data. Got it. The results of the experiments conducted throughout our project generally aligned well with the predicted values. However, as detailed in Section~\ref{sec:Application: Private and Compact Biometric Matching}, some discrepancies arose because certain assumptions of independence may not have been accurate. To address this, we modified our preHash algorithm to ensure that no vein pixel was repeated across the \(l\) iterations of preHash, and performed the first experiment from Table~\ref{tab:theoretical_parameterization_PreHash} using this new algorithm implementation. Despite these efforts, the results did not improve. This could be due to potential errors in the algorithmic implementation, or it may indicate that the chosen approach was not optimal.


\subsection{Challenges and Future Directions}
Throughout our work on this project, we encountered numerous challenges. The first significant difficulty was integrating and building upon the work of previous students. Understanding previously written code can be extremely challenging, highlighting the crucial importance of thorough documentation. We wrote a substantial amount of code for our project, ranging from algorithms such as preHash and postHash to various experimental procedures. Given the initial struggle to comprehend the existing codebase, we prioritized documenting our contributions meticulously. This included adding detailed comments and enhancing the README file to ensure clarity and ease of understanding for future developers. We believe that the comprehensive documentation we have provided will significantly benefit anyone who continues to work on this project, ensuring that our code is accessible and easy to understand.

Another significant challenge was running the experiments. With a dataset of 800 images, each experiment generated four populations of results:

- Population I: Comparisons between same biometric subjects taken from camera 1 (\(1'591\) comparisons).\\
- Population III: Comparisons between same biometric subjects taken from camera 2 (\(1'591\) comparisons).\\
- Population II: Comparisons between different biometric subjects taken from camera 1 (\(37'809\) comparisons).\\
- Population IV: Comparisons between different biometric subjects taken from camera 2 (\(37'809\) comparisons).

This amounted to a total of 78,800 comparisons per experiment, making the runtime extremely long and we spent alot of time waiting for results. We should have addressed this issue earlier but only realized the severity of the runtime problem when conducting experiments for Section~\ref{Application: Private and Compact Biometric Matching}. The extended runtime for multiple iterations of preHash and postHash combinations for each image highlighted the inefficiency. To tackle this, we revisited the previously developed pipeline code and implemented parallel processing. Additionally, we gained access to more powerful ressources (SCITAS\cite{ref4}) to run our code more efficiently. These changes drastically reduced the runtime: experiments that previously took over 24 hours to complete were reduced to just 1 hour.

However, it's important to note that despite the improved runtime, the experiments still consume a lot of resources. Some experiments, such as the last few rows of Table~\ref{tab:theoretical_parameterization_PreHash}, still take between 1 and 4 days to run. This issue can only be further addressed by refining the actual pipeline, specifically the extraction of feature vectors, and by converting the code, which is currently written in Python, to a lower-level language. This transition would likely result in more efficient code execution and reduced resource consumption. The current system requires this especially because future work will be working on a concept called "1:N matching", which adds another layer of complexity to the system, as this process involves comparing a single biometirc input against a database of multiple potential matches to identify the closest match or matches accurately.