\include{Introduction.tex}
\include{Biometric_Setting.tex}

\section{Fuzzy Hashing}
\label{sec:Fuzzy Hashing}
Fuzzy hashing, as opposed to traditional hashing, produces consistent cryptographic keys for similar but not identical inputs, enabling recognition of the same biometric trait across different instances despite slight variations. This approach ensures legitimate users are not incorrectly denied access due to minor discrepancies and protects user privacy by storing and using hashed values instead of raw biometric data, making it difficult to reverse-engineer the original data even if unauthorized access occurs. 
This section will discuss how we implemented the fuzzy hashing algorithm, its corresponding mathematical aspects and some experiments.

\subsection{PreHashing Algorithm}

The \textit{PreHash} algorithm is the first step in the fuzzy hashing process, designed to manipulate biometric templates extracted from finger vein patterns. It operates on a bitstring \(X\), representing the presence (1) or absence (0) of vein pixels across \(n\) pixels, where \(n=96'500\).

Algorithm Inputs and Outputs:
\begin{enumerate}
    \item \textbf{Inputs}: As illustrated in the pseudocode~\ref{preHash Algorithm}, the algorithm takes three main inputs:
    \begin{itemize}
        \item \textbf{A parameter m}: the number of indices to find
        \item \textbf{A bitstring X}: the feature-extracted vein patterns of a biometric capture
        \item \textbf{A key}: used to initialize a Pseudorandom Number Generator (PRNG)
    \end{itemize}
    \item \textbf{Output}: The algorithm outputs a tuple \((i_1,...,i_m)\) consisting of the \(m\) smallest indices \(i_j\)​ such that \(1 \leq i_1<...<i_m\)​ and the pixel at \(PRNGkey(i_j)\) in \(X\) is identified as a vein pixel. 
\end{enumerate}

Detailed Process of \textit{PreHash}:
\begin{itemize}
    \item \textbf{Initiallization}: Utilizing the provided key, the algorithm initializes a PRNG. This PRNG is based on the \hyperref[def:AES CTR mode]{Advanced Encryption Standard (AES) in Counter (CTR) mode}, ensuring the generation of uniform and independent pseudorandom sequences.

    \item \textbf{Nonce Generation}: A nonce in CTR mode encryption is initialized to zero to maintain simplicity and security. We opted against using a keyed hash function to generate the nonce, as it would tie the nonce to the secret key. Such a dependency would mean that both the nonce and the Pseudo-Random Number Generator (PRNG) would rely on the same key, creating a security risk by concentrating security on a single element. To avoid this, we keep the generation of the nonce separate from the key.

    \item \textbf{Pseudorandom Sequence Generation}: Upon initialization with the specified parameters—key and nonce—the PRNG operates in Counter (CTR) mode to generate a sequence of pseudo-random numbers. As illustrated in Figure~\ref{ctr encryption}, the process involves encrypting an incrementing counter combined with the nonce using the specified key. The output from this block cipher encryption is used to generate a unique stream of pseudo-random bits.
    To optimize the utilization of the cryptographic output and reduce operational overhead, the PRNG is designed to generate a full 128-bit block of pseudo-random data at a time, even though only a portion of these bits are used immediately. The generated bits are stored and consumed incrementally according to the needs of the application. For instance, if 17 bits are required, the system retrieves these bits from the stored block. Once the block is exhausted, a new block is generated.
    This mechanism ensures that all bits produced by the block cipher are utilized, enhancing efficiency and reducing the frequency of cryptographic operations. The output is then masked to retain only the most significant 17 bits to meet the project's specific requirement for representing image sizes. Specifically, a maximum of \(96,500\) pixels can be represented, which necessitates exactly 17 bits per operation \(\left(\lceil \log_2(96,500) \rceil = 17\right)\).
    The predictability and reproducibility of the sequence are entirely dictated by the chosen key. Employing the same key across sessions guarantees the generation of an identical sequence of pseudo-random numbers.

    Certainly! Here's the revised paragraph with the addition:

    \textcolor{red}{Upon initialization with the specified parameters—key and nonce—the PRNG operates in Counter (CTR) mode to generate a sequence of pseudo-random numbers. As illustrated in Figure~\ref{ctr encryption}, the process involves encrypting an incrementing counter combined with the nonce using the specified key. The output from this block cipher encryption is used to generate a unique stream of pseudo-random bits.
    To optimize the utilization of the cryptographic output and reduce operational overhead, the PRNG is designed to generate a full 128-bit block of pseudo-random data at a time, even though only a portion of these bits are used immediately. When a new block is generated, the algorithm extracts the lowest 17 bits to obtain the pseudo-random number for the current operation. This choice is aligned with the project's requirement for representing image sizes, where a maximum of \(96'500\) pixels can be represented, necessitating exactly 17 bits per operation (\(\left\lceil \log_2(96,500) \right\rceil = 17\)). These extracted bits serve as the pseudo-random number.
    Following extraction, the algorithm discards the used bits from the current block by right-shifting the block by 17 bits. This operation updates the state of the PRNG, ensuring that the consumed bits are no longer considered in subsequent operations. In the case where there are no longer 17 bits available after extraction, the algorithm generates a new pseudo-random number using the PRNG and repeats the process. This mechanism guarantees the efficient utilization of all bits produced by the block cipher, enhancing efficiency and reducing operational overhead.
    The predictability and reproducibility of the sequence are entirely dictated by the chosen key. Employing the same key across sessions guarantees the generation of an identical sequence of pseudo-random numbers. }
    
     

    \begin{figure}[!h]
        \centering
        \includegraphics[width=1\linewidth]{latex-img/CTR_encryption.png}
        \caption{Counter (CTR) mode encryption}
        \label{ctr encryption}
    \end{figure}
    

    \item \textbf{Selection of Indices}: The algorithm iterates through the generated pseudorandom sequence, selecting the first \(m\) indices corresponding to vein pixels in the biometric template \(X\). This selection process involves a careful mechanism to ensure the uniqueness and proper ordering of indices.

    \item \textbf{Corner Cases}: In scenarios where there are no veins present in the image, the algorithm incorporates a built-in mechanism to address such situations. Prior to initiating the prehash process, it assesses whether at least one vein is present in the image. If no veins are detected, the algorithm suspends further execution, preventing an infinite loop.

\end{itemize}


\begin{algorithm}
\begin{algorithmic}[1]
\caption{\textit{preHash} Algorithm}
\label{preHash Algorithm}
\Function{\text{preHash}$_\text{key}^m$}{$X$}
\State \hfill $\triangleright$ $L \gets \lceil \log_2(n) \rceil$
\State Initialize PRNG using key for $L$ bits
\State $i \gets 0$
\For{$j \gets 1 \text{ to } m$}
    \Repeat
        \Repeat
            \State $k \gets \text{PRNG}$
        \Until{$1 \leq k \leq n$}
        \State $i \gets i + 1$
    \Until{$X_k$ is a vein pixel}
    \State $i_j \gets k$
    \EndFor 
\State \Return{$i_1, \ldots, i_m$}
\EndFunction
\end{algorithmic}
\end{algorithm}

The algorithm, preHash, illustrated in Figure~\ref{preHash Algorithm} generates \(m\) smallest indices, denoted by \(i_j\), such that \(j\in{[1, m]} \) and \(1 <= i_1 < ... < i_m\), where each \(i_j\) correponds to an index such that \(X_{PRNG_{key}(i_j)} = 1\). It achieves this by rigorously verifying that the numbers produced by PRNG (PRNG[i]) stay within the specified bounds \(\text{PRNG}[i] \in (0, n] \text{ for } i \in [0, m]\).

\subsection{Assessing Similarity of Biometric Inputs After PreHash Application}
\label{sec:mu}

After the finger images are processed through the pipeline described in Pipeline~\ref{pipeline_simon} to extract their feature vectors, and the \textit{preHash} algorithm is applied, the outcome is a set of indices that fall within the inclusive range \(\text{PRNG}[i] \in (0, n] \text{ for } i \in [0, m]\), effectively mapping each selected feature to a unique index within the feature vector's length.

In the context of a simplified scenario where the hash length parameter (\(m\)) is set to \(1\), implying the generation of a single-index hash, and assuming a randomly chosen key for the \textit{preHash} algorithm, along with \(k\) representing a uniformly distributed random index, the probability that the \textit{preHash} operation yields the same index for two different inputs \(X\) and \(Y\) can be mathematically delineated as follows:

\begin{equation} \label{eq:preHash1}
    \begin{aligned}
        Pr[preHash_{key}^1(X) = preHash_{key}^1(Y)] &= \sum_{i > 0} Pr[preHash_{key}^1(X)]\\
        &= preHash_{key}^1(Y)\\
        &= \sum_{i > 0} Pr[X_k = Y_k = 0]^{i - 1} Pr[X_k = Y_k = 1]\\
        &= \frac{Pr[X_k = Y_k = 1]}{1 - Pr[X_k = Y_k = 0]}\\
        &= \frac{HW(X \land Y)}{HW(X) + HW(Y) - HW(X \land Y)}\\
        &= \frac{1}{\frac{1}{Score(X, Y)} - 1}
    \end{aligned}
\end{equation}

This equation encapsulates the likelihood of two images, \(X\) and \(Y\), having their singular hash index coincide, based on the presence of matching features identified by the algorithm. The final form of the equation relates the probability to the scoring function between \(X\) and \(Y\), inversely proportional to the score minus one.

It is noticed that there is a direct link with the Miura matching score that is of interest. The direct link between the \textit{preHash} algorithm's outcomes and the Miura matching score lies in their shared foundation of evaluating biometric similarities. Specifically, both methodologies utilize Hamming weight and bitwise operations to assess the overlap between biometric samples, such as finger vein patterns. The \textit{preHash} algorithm, through its probabilistic formula, quantifies the likelihood of matching indices based on feature presence, closely paralleling the Miura score's approach of comparing binary patterns to derive a similarity score. The above computation can also be expressed as follows:

\begin{equation} \label{eq:preHash2}
    \begin{aligned}
        Pr[preHash_{key}^1(\bar{X}) = preHash_{key}^1(\bar{Y})] &= \frac{Pr[\bar{X}_k = \bar{Y}_k = 1]}{1 - Pr[\bar{X}_k = \bar{Y}_k = 0]}\\
        &= \frac{\frac{Pr[X_k = 1] + Pr[Y_k = 1]}{2} - \frac{1}{2}Pr[\bar{X}_l \neq \bar{Y}_k]}{\frac{Pr[X_k = 1] + Pr[Y_k = 1]}{2} + \frac{1}{2}Pr[\bar{X}_l \neq \bar{Y}_k]}\\
    \end{aligned}
\end{equation}

The following approximations are made, inspired by equations p (\ref{eq:proba}) and $\delta$ (\ref{eq:delta}):

\begin{equation}
    E\left(\frac{\frac{Pr[X_k = 1] + Pr[Y_k = 1]}{2} - \frac{1}{2}Pr[\bar{X}_l \neq \bar{Y}_k]}{\frac{Pr[X_k = 1] + Pr[Y_k = 1]}{2} + \frac{1}{2}Pr[\bar{X}_l \neq \bar{Y}_k]}\right) \approx \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}}
\end{equation}

The core of this approximation revolves around the expectation formula, which integrates probabilities of feature presence \(Pr[X_k=1]+Pr[Y_k=1]\) and the likelihood of discrepancies between \(X\) and \(Y\), \(Pr[\bar{X}_l \neq \bar{Y}_k]\). This formula essentially aims to quantify the similarity between two biometric samples by considering both the concurrence of features and the instances where they diverge.

Hence for (\(X\), \(Y\)) random,

\begin{equation}
    \label{eq:mu_leq}
    Pr[preHash_{key}^1(offset_X * X) = preHash_{key}^1(offset_Y * Y)] \leq \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}}
\end{equation}

where equality is reached for the optimal offset translations. 

Depending on the distribution of (\(X\), \(Y\)), it is denoted

\begin{equation} \label{eq:mu}
    \mu = \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}}
\end{equation}

The following figures are provided:

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.25}\begin{tabular}{|c|c|c|}
        \hline
        $\mu_{same}$ & $\mu_{diff}$ & $\mu_{indep}$\\
        \hline
        $23.02\%$ & $7.72\%$ & $1.38\%$\\
        \hline
    \end{tabular}
\caption{Comparison of Distributions: $\delta_{same}$, $\delta_{diff}$, and $\delta_{indep}$}
\end{table}

Finally, it is observed that

\begin{equation}
    Pr[preHash_{key}^m(offset_X * X) = preHash_{key}^m(offset_Y * Y)] \leq \mu^m
\end{equation}

where equality is reached for the optimal offset translations.

%This includes determining the upper limits for the probabilities of similarity between different finger veins processed through the same fuzzy hashing parameters. 
\subsection{Experimental Derivation of the Probabilities \(\mu_{\text{same}}, \mu_{\text{diff}}, \mu_{\text{indep}}\)}

In Section~\ref{sec:mu}, we have laid out the mathematical framework that defines the probability \(\mu\). This probability is crucial in our fuzzy hashing approach, as it quantifies the matching likelihood after applying the optimal offset translations to the biometric captures. The application of these optimal offsets is an important preprocessing step before hashing the dataset, ensuring that the biometric features are accurately aligned for comparison.

We will proceed to evaluate these theoretical results experimentally, leveraging the same comprehensive database that has been utilized in our prior research, as explained in Section~\ref{sec:delta}. This experimental assessment will enable us to substantiate the theoretical predictions of \(\mu_{\text{same}}\) and \(\mu_{\text{diff}}\).

In the experimental setup, \(\mu_{\text{same}}\) refers to the probability that a hash function, when applied to two biometric captures of the same finger, will yield the same index. This is the measure of success in correctly identifying matches within the same individual's biometric captures. Conversely, \(\mu_{\text{diff}}\) denotes the probability that the same hash function will produce different indices for biometric captures from different individuals, representing the ability to distinguish between different people's biometric data. Lastly, \(\mu_{\text{indep}}\) indicates the probability of a match in the case where the biometric captures are completely independent, serving as a baseline for random chance.

The computation of \( \mu_{\text{indep}} \) is relatively straightforward, having determined the value of \( p \) and \(\delta_{\text{indep}}\). The formula for \( \mu_{\text{indep}} \) relies solely on \( p \) and \(\delta_{\text{indep}}\), allowing us to express it as:

\[
    \mu_{\text{indep}} = \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}} \approx 0.0138 = 1.38\%
\]

For \(\mu_{\text{same}}\) and \(\mu_{\text{diff}}\), our experimental protocol adhered to the established optimal pipeline (refer to Figure~\ref{pipeline_simon}) to process the images in our dataset. Following the alignment of images through the computation of optimal offsets (referred to as Miura Matching), our fuzzy hashing function was applied to the extraced features. In alignment with the premise that \(\mu\) is computed under the condition where the hash function's output is a single index (\(m=1\)).

\begin{enumerate}
    \item \textbf{Pairwise Comparison}: We performed a pairwise comparison for each image pair within the dataset after applying the optimal offsets to ensure proper alignment. The fuzzy hashing function, tailored to output a single index per image, was utilized to hash the images.

    \item \textbf{Hamming Distance Computation}: We calculated the Hamming distance between the indices obtained from the fuzzy hashing. The distances were binary:
    \begin{itemize}
        \item A Hamming distance of 0 indicates the indices are identical, suggesting a match.
        \item A Hamming distance of 1 indicates differing indices, suggesting a non-match.
    \end{itemize}
    
    \item \textbf{Statistical Analysis and Probability Computation}:
    \begin{itemize}
        \item \textbf{\(\mu_{\text{same}}\)}: We computed \(\mu_{\text{same}}\) by averaging the number of Hamming distances that resulted in a match (Hamming distances of 0) of the intra-individual comparisons, which furnished us with the following probability:

        \[ \mu_{\text{same}} \approx 0.2302 = 23.02\%\] 
        
        \item \textbf{\(\mu_{\text{diff}}\)}: Similarly, \(\mu_{\text{diff}}\) was determined by averaging the number of Hamming distances that resulted in a match (Hamming distances of 0) of the inter-individual comparisons, which furnished us with the following probability:

        \[ \mu_{\text{diff}} \approx 0.0772 = 7.72\% \]
    \end{itemize}
\end{enumerate}

In our analysis of the dataset at hand featuring biometric data from 20 unique individuals, we have quantified the performance of the preHash function in distinguishing between identical and different biometric subjects. Specifically, when evaluating samples from the same individual, the probability that the preHash function will produce the same index is approximately \(23\%\). Conversely, when the function is applied to samples from different individuals, the probability of a match in the indices drops to roughly \(7.7\%\). This indicates that the likelihood of mistakenly identifying different individuals as the same is about three times lower than correctly matching samples from the same individual, demonstrating the preHash function's relative efficacy in biometric differentiation within this specific dataset.

It would be valuable to conduct an experimental investigation to determine the effect of omitting the Miura Matching step (finding optimal offsets before comparing images), which serves as a post-alignment phase in our pipeline, on the coincidence probabilities of the indices. Our interest in conducting this verification stems from the contents of Equation~\ref{eq:mu_leq}. Should some other non-optimal offsets be used, or should the Miura Matching phase be excluded entirely from our process, it is anticipated that the probabilities of matching indices should be less than or equal to the values determined for \(\mu_{\text{same}}\) and \(\mu_{\text{diff}}\), respectively.

% A faire cette experience!!! Juste enlever le miura matching complètement et observer mu_same et mu_diff





