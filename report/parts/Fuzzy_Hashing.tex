\include{Introduction.tex}
\include{Biometric_Setting.tex}

\section{Fuzzy Hashing}
\label{sec:Fuzzy Hashing}
Fuzzy hashing, as opposed to traditional hashing, produces consistent cryptographic keys for similar but not identical inputs, enabling recognition of the same biometric trait across different instances despite slight variations. This approach ensures legitimate users are not incorrectly denied access due to minor discrepancies and protects user privacy by storing and using hashed values instead of raw biometric data, making it difficult to reverse-engineer the original data even if unauthorized access occurs. 
This section will discuss how we implemented the fuzzy hashing algorithm, its corresponding mathematical aspects and some experiments.

\subsection{PreHash Algorithm}

The preHash algorithm is the first step in the fuzzy hashing process, designed to manipulate biometric templates extracted from finger vein patterns. It operates on a bitstring \(X\), representing the presence (1) or absence (0) of vein pixels across \(n\) pixels, where \(n=90'240\).

Algorithm Inputs and Outputs:
\begin{enumerate}
    \item \textbf{Inputs}: As illustrated in the pseudocode~\ref{preHash Algorithm}, the algorithm takes three main inputs:
    \begin{itemize}
        \item \textbf{A parameter m}: the number of indices to find
        \item \textbf{A bitstring X}: the feature-extracted vein patterns of a biometric capture
        \item \textbf{A key}: used to initialize a Pseudorandom Number Generator (PRNG)
    \end{itemize}
    \item \textbf{Output}: The algorithm outputs a tuple \((i_1,...,i_m)\) consisting of the \(m\) smallest indices \(i_j\)​ such that \(1 \leq i_1<...<i_m\)​ and the pixel at \(PRNG_{key(i_j)}\) in \(X\) is identified as a vein pixel. 
\end{enumerate}

Detailed Process of preHash:
\begin{itemize}
    \item \textbf{Initiallization}: Utilizing the provided key, the algorithm initializes a PRNG. This PRNG is based on the \hyperref[def:AES CTR mode]{Advanced Encryption Standard (AES) in Counter (CTR) mode}, ensuring the generation of uniform and independent pseudorandom sequences.

    \item \textbf{Nonce Generation}: A nonce in CTR mode encryption is initialized to zero to maintain simplicity and security. We opted against using a keyed hash function to generate the nonce, as it would tie the nonce to the secret key. Such a dependency would mean that both the nonce and the Pseudo-Random Number Generator (PRNG) would rely on the same key, creating a security risk by concentrating security on a single element. To avoid this, we keep the generation of the nonce separate from the key.

    \item \textbf{Pseudorandom Sequence Generation}: Upon initialization with the specified parameters—key and nonce—the PRNG operates in Counter (CTR) mode to generate a sequence of pseudo-random numbers. As illustrated in Figure~\ref{ctr encryption}, the process involves encrypting an incrementing counter combined with the nonce using the specified key. The output from this block cipher encryption is used to generate a unique stream of pseudo-random bits.
    To optimize the utilization of the cryptographic output and reduce operational overhead, the PRNG is designed to generate a full 128-bit block of pseudo-random data at a time, even though only a portion of these bits are used immediately. When a new block is generated, the algorithm extracts the lowest 17 bits to obtain the pseudo-random number for the current operation. This choice is aligned with the project's requirement for representing image sizes, where a maximum of \(90'240\) pixels can be represented, necessitating exactly 17 bits per operation (\(\left\lceil \log_2(90'240) \right\rceil = 17\)). These extracted bits serve as the pseudo-random number.
    Following extraction, the algorithm discards the used bits from the current block by right-shifting the block by 17 bits. This operation updates the state of the PRNG, ensuring that the consumed bits are no longer considered in subsequent operations. In the case where there are no longer 17 bits available after extraction, the algorithm generates a new pseudo-random number using the PRNG and repeats the process. This mechanism guarantees the efficient utilization of all bits produced by the block cipher, enhancing efficiency and reducing operational overhead.
    The predictability and reproducibility of the sequence are entirely dictated by the chosen key. Employing the same key across sessions guarantees the generation of an identical sequence of pseudo-random numbers.
    
     

    \begin{figure}[!h]
        \centering
        \includegraphics[width=1\linewidth]{latex-img/CTR_encryption.png}
        \caption{Counter (CTR) mode encryption}
        \label{ctr encryption}
    \end{figure}
    

    \item \textbf{Selection of Indices}: The algorithm iterates through the generated pseudorandom sequence, selecting the first \(m\) indices corresponding to vein pixels in the biometric template \(X\). This selection process involves a careful mechanism to ensure the uniqueness and proper ordering of indices.

    \item \textbf{Corner Cases}: In scenarios where there are no veins present in the image, the algorithm incorporates a built-in mechanism to address such situations. Prior to initiating the preHash process, it assesses whether at least one vein is present in the image. If no veins are detected, the algorithm suspends further execution, preventing an infinite loop.

\end{itemize}


\begin{algorithm}
\begin{algorithmic}[1]
\caption{preHash Algorithm}
\label{preHash Algorithm}
\Function{preHash$_\text{key}^m$}{$X$}
\State \hfill $\triangleright$ $L \gets \lceil \log_2(n) \rceil$
\State Initialize PRNG using key for $L$ bits
\State $i \gets 0$
\For{$j \gets 1 \text{ to } m$}
    \Repeat
        \Repeat
            \State $k \gets \text{PRNG}$
        \Until{$1 \leq k \leq n$}
        \State $i \gets i + 1$
    \Until{$X_k$ is a vein pixel}
    \State $i_j \gets k$
    \EndFor 
\State \Return{$i_1, \ldots, i_m$}
\EndFunction
\end{algorithmic}
\end{algorithm}

The algorithm, preHash, illustrated in Figure~\ref{preHash Algorithm} generates \(m\) smallest indices, denoted by \(i_j\), such that \(j\in{[1, m]} \) and \(1 \leq i_1 < ... < i_m\), where each \(i_j\) correponds to an index such that \(X_{PRNG_{key}(i_j)} = 1\). It achieves this by rigorously verifying that the numbers produced by PRNG stay within the specified bounds, i.e. \(\text{PRNG}[i] \in (0, n] \text{ for all } i \in [0, m]\).

\subsection{Assessing Similarity of Biometric Inputs After PreHash Application}
\label{sec:mu}

After the finger images are processed through the pipeline described in Figure~\ref{pipeline_simon} to extract their feature vectors, and the preHash algorithm is applied, the outcome is a set of indices that fall within the inclusive range \(\text{PRNG}[i] \in (0, n] \text{ for } i \in [0, m]\), effectively mapping each selected feature to a unique index within the feature vector's length.

In the context of a simplified scenario where the hash length parameter (\(m\)) is set to \(1\), implying the generation of a single-index hash, and assuming a randomly chosen key for the preHash algorithm, along with \(k\) representing a uniformly distributed random index, the probability that the preHash operation yields the same index for two different fixed inputs \(X\) and \(Y\) can be mathematically delineated as follows:


\begin{equation} \label{eq:preHash1}
    \begin{aligned}
        Pr[preHash_{key}^1(X) = preHash_{key}^1(Y)] &= \sum_{i > 0} Pr[preHash_{key}^1(X) = preHash_{key}^1(Y) = i]\\
        &= \sum_{i > 0} Pr[X_k = Y_k = 0]^{i - 1} Pr[X_k = Y_k = 1]\\
        &= \frac{Pr[X_k = Y_k = 1]}{1 - Pr[X_k = Y_k = 0]}\\
        &= \frac{HW(X \land Y)}{HW(X) + HW(Y) - HW(X \land Y)}\\
        &= \frac{1}{\frac{1}{Score(X, Y)} - 1}
    \end{aligned}
\end{equation}

This equation encapsulates the likelihood of two images, \(X\) and \(Y\), having their singular hash index coincide, based on the presence of matching features identified by the algorithm. The final form of the equation relates the probability to the scoring function between \(X\) and \(Y\), inversely proportional to the score minus one.

It is noticed that there is a direct link with the Miura matching score that is of interest. The direct link between the preHash algorithm's outcomes and the Miura matching score lies in their shared foundation of evaluating biometric similarities. Specifically, both methodologies utilize Hamming weight and bitwise operations to assess the overlap between biometric samples, such as finger vein patterns. The preHash algorithm, through its probabilistic formula, quantifies the likelihood of matching indices based on feature presence, closely paralleling the Miura score's approach of comparing binary patterns to derive a similarity score. The above computation can also be expressed as follows:

\begin{equation} \label{eq:preHash2}
    \begin{aligned}
        Pr[preHash_{key}^1(\bar{X}) = preHash_{key}^1(\bar{Y})] &= \frac{Pr[\bar{X}_k = \bar{Y}_k = 1]}{1 - Pr[\bar{X}_k = \bar{Y}_k = 0]}\\
        &= \frac{\frac{Pr[X_k = 1] + Pr[Y_k = 1]}{2} - \frac{1}{2}Pr[\bar{X}_k \neq \bar{Y}_k]}{\frac{Pr[X_k = 1] + Pr[Y_k = 1]}{2} + \frac{1}{2}Pr[\bar{X}_k \neq \bar{Y}_k]}\\
    \end{aligned}
\end{equation}

The following approximations are made, inspired by equations \(p\) (\ref{eq:p}) and $\delta$ (\ref{eq:delta}):

\begin{equation}
    \mathbb{E}\left(\frac{\frac{Pr[X_k = 1] + Pr[Y_k = 1]}{2} - \frac{1}{2}Pr[\bar{X}_k \neq \bar{Y}_k]}{\frac{Pr[X_k = 1] + Pr[Y_k = 1]}{2} + \frac{1}{2}Pr[\bar{X}_k \neq \bar{Y}_k]}\right) \approx \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}}
\end{equation}

The core of this approximation revolves around the expectation formula, which integrates probabilities of feature presence \(Pr[X_k=1]+Pr[Y_k=1]\) and the likelihood of discrepancies between \(\bar{X}\) and \(\bar{Y}\), \(Pr[\bar{X}_k \neq \bar{Y}_k]\). This formula essentially aims to quantify the similarity between two biometric samples by considering both the concurrence of features and the instances where they diverge.

To achieve a more accurate approximation of the expected value of Equation \ref{eq:preHash2}, we define the following parameters and employ a Taylor series expansion of the function \(f(A,B,C) = \frac{A + B - C}{A + B + C}\) around the point \((p, p, \delta)\). This method allows us to incorporate not only the means but also the variances and covariances of the variables involved.

\[
A = \frac{HW(\bar{X})}{n}, \quad B = \frac{HW(\bar{Y})}{n}, \quad C = \frac{d_H(\bar{X}, \bar{Y})}{n}
\]
where \(HW(\cdot)\) denotes the Hamming weight and \(\frac{d_H(\cdot, \cdot)}{n}\) represents the normalized Hamming distance. Given these definitions, we know the expected values and variances from Section~\ref{sec:bio_setting} :
\[
    \mathbb{E}(A) = \mathbb{E}(B) = p^\text{obs}, \quad \mathbb{E}(C) = \delta^\text{obs}, \quad \mathbb{V}(A) = \mathbb{V}(B) = ({\sigma^\text{obs}_p})^2, \quad \mathbb{V}(C) = ({\sigma^\text{obs}_\delta})^2
\]

Next, we perform a Taylor series expansion of \(f(A,B,C)\) up to the second order around the point \((p, p, \delta)\). The first-order partial derivatives are calculated as follows:
\[
f'_A = \frac{2C}{(A + B + C)^2}, \quad f'_B = \frac{2C}{(A + B + C)^2}, \quad f'_C = \frac{-2(A + B)}{(A + B + C)^2}
\]

The second-order partial derivatives are:
\[
f''_{AA} = f''_{BB} = f''_{AB} = \frac{-4C}{(A + B + C)^3}, \quad f''_{CC} = \frac{4(A + B)}{(A + B + C)^3}, \quad f''_{AC} = f''_{BC} = \frac{-2(A + B - C)}{(A + B + C)^3}
\]

By substituting \(A = p\), \(B = p\), and \(C = \delta\) into these second order derivatives, we obtain:

\[
f''_{AA} = f''_{BB} = f''_{AB} = \frac{-4\delta}{(2p + \delta)^3}, \quad f''_{CC} = \frac{8p}{(2p + \delta)^3}, \quad f''_{AC} = f''_{BC} = \frac{2(2p - \delta)}{(2p + \delta)^3}
\]

Finally, the second order Taylor expansion of \(f(A,B,C)\) around \((p,p,\delta)\) is given by:
\[
f(A,B,C) \approx f(p,p,\delta) + f'_A(p,p,\delta)(A - p) + f'_B(p,p,\delta)(B - p) + f'_C(p,p,\delta)(C - \delta)
\]
\[
+ \frac{1}{2} \left[ f''_{AA}(p,p,\delta)(A - p)^2 + f''_{BB}(p,p,\delta)(B - p)^2 + f''_{CC}(p,p,\delta)(C - \delta)^2 \right.
\]
\[
+ 2f''_{AB}(p,p,\delta)(A - p)(B - p) + 2f''_{AC}(p,p,\delta)(A - p)(C - \delta) + 2f''_{BC}(p,p,\delta)(B - p)(C - \delta)
\]\\
To approximate the expected value \( E(f(A,B,C)) \), we use the following relations:
\[
    \mathbb{E}(A - p) = 0, \quad \mathbb{E}(B - p) = 0, \quad \mathbb{E}(C - \delta) = 0
\]
\[
    \mathbb{V}(A) = \mathbb{V}(B) = \mathbb{E}((A - p)²), \quad \mathbb{V}(C) = \mathbb{E}((C - \delta)²)
\]
\[
    \text{Cov}(A, B) = \mathbb{E}((A - p)(B - p)), \quad \text{Cov}(B, C) = \mathbb{E}((B - p)(C - \delta))
\]
\[
    \text{Cov}(A, C) = \mathbb{E}((A - p)(C - \delta))
\]

And we hence have:
\[
    \mathbb{E}(f(A,B,C)) \approx \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}} + \frac{1}{2} \left[ f''_{AA}(p,p,\delta)\mathbb{V}(A) + f''_{BB}(p,p,\delta)\mathbb{V}(B) + f''_{CC}(p,p,\delta)\mathbb{V}(C) \right.
\]
\[
+ 2f''_{AB}(p,p,\delta) \text{Cov}(A,B) + 2f''_{AC}(p,p,\delta) \text{Cov}(A,C) + 2f''_{BC}(p,p,\delta) \text{Cov}(B,C)]
\]

Simplifying with our values and notations we get:

\begin{equation}
    \begin{aligned}
    \mathbb{E}(f(A,B,C)) &\approx \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}} + \frac{1}{2} \left[ \frac{-4\delta}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_p})^2 + \frac{-4\delta}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_p})^2 \right. \\[2mm]
    & \left. + \frac{8p}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_\delta})^2 + 2\cdot\frac{-4\delta}{(2p + \delta)^3}\cdot\text{Cov}(A,B) \right. \\[2mm]
    & \left. + 2\cdot\frac{2(2p - \delta)}{(2p + \delta)^3}\cdot\text{Cov}(A,C) + 2\cdot\frac{2(2p - \delta)}{(2p + \delta)^3}\cdot\text{Cov}(B,C) \right]\\
    \end{aligned}
    \label{eq:mu_approximation}
\end{equation}

Hence for (\(X\), \(Y\)) random,
\begin{equation}
    \label{eq:mu_leq}
    Pr[preHash_{key}^1(offset_X * X) = preHash_{key}^1(offset_Y * Y)] \leq \mathbb{E}(f(A,B,C))
\end{equation}
where equality is reached for the optimal offset translations.\\

Depending on the distribution of (\(X\), \(Y\)), it is denoted

\begin{equation} \label{eq:mu}
    \mu = \mathbb{E}(f(A,B,C))
\end{equation}\\

In order to assess the theoretical approximations of $\mu_{same}$, $\mu_{diff}$, and the value of $\mu_{indep}$, we can easily replace the different values in Equation~\ref{eq:mu_approximation} with the corresponding values calculated in the previous sections. We demonstrate how this is done for \( \mu_{\text{indep}}^{obs} \). Using \( p^{obs} \), \(\delta_{\text{indep}}^{obs}\), and their respective standard deviations, we aim to calculate \(\mathbb{E}(f(A,B,C))\). This calculation is based on \(X\) and \(Y\) comprising \(2n\) independent random bits with an expected value of \(p\), without applying the optimal offset to obtain \(\bar{X}\) and \(\bar{Y}\). Equation~\ref{eq:mu_approximation} is simplified because the covariance between \(A\) and \(B\) is zero (because of independence).

For clarity, note that in the following equation:

- \(p\) corresponds to \(p^{obs}\),\\
- \(\delta\) corresponds to \(\delta_{\text{indep}}^{obs}\),\\
- \(\sigma_p\) corresponds to \(\sigma_p^{obs}\), and\\
- \(\sigma_\delta\) corresponds to \(\sigma_{\delta_{\text{indep}}}^{\text{obs}}\).

This allows us to express \( \mu_{\text{indep}}^{obs} \) as follows:

\begin{equation}
    \begin{aligned}
    \mu_{\text{indep}}^{obs} = \mathbb{E}(f(A,B,C)) &\approx \quad \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}} + \frac{1}{2} \left[ \frac{-4\delta}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_p})^2 + \frac{-4\delta}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_p})^2 \right. \\[2mm]
    & \left. + \frac{8p}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_\delta})^2 + 2\cdot\frac{2(2p - \delta)}{(2p + \delta)^3}\cdot\text{Cov}(A,C) \right. \\[2mm]
    & \left. + 2\cdot\frac{2(2p - \delta)}{(2p + \delta)^3}\cdot\text{Cov}(B,C) \right]\\[6mm]
    & = \num{1.78e-2} + \frac{1}{2} \left[ -2\cdot\num{2.6e-3} + \num{1.5e-4}  \right. \\[2mm]
    & \left. + 2\cdot\num{1.31e-6} \right] \\[6mm]
    & \approx 0.01536 = 1.54\%
    \end{aligned}
    \label{eq:mu_indep}
\end{equation}\\

We proceed in a similar way to assess the theoretical values of $\mu_{same}$ and $\mu_{diff}$, except in this case the covariances are non-zero and have been calculated by observing the correlation between the different random variables (\(A\), \(B\), \(C\)) in the experimental results obtained in Section~\ref{sec:bio_setting}. That is, we calculated the covariance matrix between the following random variables:
\[
A = \frac{HW(\bar{X})}{n}, \quad B = \frac{HW(\bar{Y})}{n}, \quad C = \frac{d_H(\bar{X}, \bar{Y})}{n}
\]

We obtained the following covariance matrices from the experiments held to find \(\delta^{obs}_{diff}\) and \(\delta^{obs}_{same}\):

\[
\begin{array}{c|ccc}
           & \text{A} & \text{B} & \text{C} \\
\hline
\text{A} & 0.000025 & 0.000022 & 0.000036 \\
\text{B} & 0.000022 & 0.000025 & 0.000036 \\
\text{C} & 0.000036 & 0.000036 & 0.000106 \\
\end{array}
\]
\begin{center}\text{Covariance Matrix for Same Distribution}\end{center}


\[
\begin{array}{c|ccc}
           & \text{A} & \text{B} & \text{C} \\
\hline
\text{A} & 0.000025 & 0.000006 & 0.000026 \\
\text{B} & 0.000006 & 0.000026 & 0.000027 \\
\text{C}  & 0.000026 & 0.000027 & 0.000046 \\
\end{array}
\]
\begin{center}\text{Covariance Matrix for Different Distribution}\end{center}

We have the following equations for \(\mu_{same}\) and \(\mu_{diff}\). Depending on the distribution (same or different), the variables \(p\), \(\delta\), \(\sigma_p\), and \(\sigma_\delta\) correspond to their observed counterparts as indicated:

- \(p \rightarrow p^{obs}\)\\
- \(\delta \rightarrow \delta_{\text{same/diff}}^{obs}\)\\
- \(\sigma_p \rightarrow \sigma_p^{obs}\)\\
- \(\sigma_\delta \rightarrow \sigma_{\delta_{\text{same/diff}}}^{\text{obs}}\)\\

\begin{equation}
    \begin{aligned}
    \mu_{\text{same}}^{obs} = \mathbb{E}(f(A,B,C)) &\approx \quad \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}} + \frac{1}{2} \left[ \frac{-4\delta}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_p})^2 + \frac{-4\delta}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_p})^2 \right. \\[2mm]
    & \left. + \frac{8p}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_\delta})^2 + 2\cdot\frac{-4\delta}{(2p + \delta)^3}\cdot\text{Cov}(A,B) \right. \\[2mm] 
    & \left. + 2\cdot\frac{2(2p - \delta)}{(2p + \delta)^3}\cdot\text{Cov}(A,C) + 2\cdot\frac{2(2p - \delta)}{(2p + \delta)^3}\cdot\text{Cov}(B,C) \right]\\[10mm]
    & = \num{2.14e-1} + \frac{1}{2} \left[ -2\cdot\num{2.9e-3} + \num{1.9e-2}  \right. \\[2mm]
    & \left. - \num{5.1e-3} + 2\cdot\num{2.3e-3} \right] \\[10mm]
    & \approx 0.22048 = 22.05\%
    \end{aligned}
    \label{eq:mu_same}
\end{equation}

\begin{equation}
    \begin{aligned}
    \mu_{\text{diff}}^{obs} = \mathbb{E}(f(A,B,C)) &\approx \quad \frac{p - \frac{\delta}{2}}{p + \frac{\delta}{2}} + \frac{1}{2} \left[ \frac{-4\delta}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_p})^2 + \frac{-4\delta}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_p})^2 \right. \\[2mm]
    & \left. + \frac{8p}{(2p + \delta)^3}\cdot({\sigma^\text{obs}_\delta})^2 + 2\cdot\frac{-4\delta}{(2p + \delta)^3}\cdot\text{Cov}(A,B) \right. \\[2mm] 
    & \left. + 2\cdot\frac{2(2p - \delta)}{(2p + \delta)^3}\cdot\text{Cov}(A,C) + 2\cdot\frac{2(2p - \delta)}{(2p + \delta)^3}\cdot\text{Cov}(B,C) \right]\\[10mm]
    & = \num{7.99e-2} + \frac{1}{2} \left[ -2\cdot\num{2.7e-3} + \num{5.8e-3}  \right. \\[2mm]
    & \left. - \num{1.3e-3} + \num{4.9e-4} + \num{5e-4} \right] \\[10mm]
    & \approx 0.07993 = 7.99\%
    \end{aligned}
    \label{eq:mu_diff}
\end{equation}

Finally, the following figures are provided:

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.25}\begin{tabular}{|c|c|c|}
        \hline
        $\mu_{same}$ & $\mu_{diff}$ & $\mu_{indep}$\\
        \hline
        $21.41\%$ & $7.99\%$ & $1.78\%$\\
        \hline
    \end{tabular}
\caption{Theoretical Approximations without Taylor Series Expansion: $\mu_{same}$, $\mu_{diff}$, and $\mu_{indep}$}
\label{tab:mu_approx}
\end{table}


\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.25}\begin{tabular}{|c|c|c|}
        \hline
        $\mu_{same}$ & $\mu_{diff}$ & $\mu_{indep}$\\
        \hline
        $22.05\%$ & $7.99\%$ & $1.54\%$\\
        \hline
    \end{tabular}
\caption{Theoretical Approximations with Taylor Series Expansion: $\mu_{same}$, $\mu_{diff}$, and $\mu_{indep}$}
\label{tab:approximation_mu_taylor}
\end{table}

We can note that there is not a major difference between approximations from Table~\ref{tab:approximation_mu_taylor} and Table~\ref{tab:mu_approx}. Yet, the approximation from Table~\ref{tab:approximation_mu_taylor} yields extremely close values to the experimental results from subsection~\ref{sec:experiment_fuzzy_hash}.

Finally, it is observed that:

\begin{equation}
    Pr[preHash_{key}^m(offset_X * X) = preHash_{key}^m(offset_Y * Y)] \leq \mu^m
\end{equation}
\\
where equality is reached for the optimal offset translations.

\subsection{Experimental Derivation of the Probabilities \(\mu_{\text{same}}^{\text{obs}}, \mu_{\text{diff}}^{\text{obs}}, \mu_{\text{indep}}^{\text{obs}}\)}
\label{sec:experiment_fuzzy_hash}

In Section~\ref{sec:mu}, we have laid out the mathematical framework that defines the upper bound \(\mu\). This probability is crucial in our fuzzy hashing approach, as it quantifies the matching likelihood after applying the optimal offset translations to the biometric captures. The application of these optimal offsets is an important preprocessing step before hashing the dataset, ensuring that the biometric features are accurately aligned for comparison.

We will proceed to evaluate these theoretical approximations (Table~\ref{tab:mu_approx}) experimentally, leveraging the same comprehensive database that has been utilized in our prior research, as explained in Section~\ref{sec:delta}. This experimental assessment will enable us to substantiate the theoretical predictions of \(\mu_{\text{same}}\) and \(\mu_{\text{diff}}\).

In the experimental setup, \(\mu^{obs}_{\text{same}}\) refers to the probability that a hash function, when applied to two biometric captures of the same finger, will yield the same index. This is the measure of success in correctly identifying matches within the same individual's biometric captures. Conversely, \(\mu^{obs}_{\text{diff}}\) denotes the probability that the same hash function will produce different indices for biometric captures from different individuals, representing the ability to distinguish between different people's biometric data. Lastly, \(\mu^{obs}_{\text{indep}}\) indicates the probability of a match in the case where the biometric captures are completely independent, serving as a baseline for random chance, which has already been calculated (see Equation~\ref{eq:mu_indep})

For \(\mu_{\text{same}}^{obs}\) and \(\mu_{\text{diff}}^{obs}\), our experimental protocol adhered to the established optimal pipeline (refer to Figure~\ref{pipeline_simon}) to process the images in our dataset. Following the alignment of images through the computation of optimal offsets (referred to as Miura Matching), our fuzzy hashing function was applied to the extraced features. In alignment with the premise that \(\mu\) is computed under the condition where the hash function's output is a single index (\(m=1\)).

\begin{enumerate}
    \item \textbf{Pairwise Comparison}: We performed a pairwise comparison for each image pair within the dataset after applying the optimal offsets to ensure proper alignment. The fuzzy hashing function, tailored to output a single index per image, was utilized to hash the images.

    \item \textbf{Hamming Distance Computation}: We calculated the Hamming distance between the indices obtained from the fuzzy hashing. The distances were binary:
    \begin{itemize}
        \item A Hamming distance of 0 indicates the indices are identical, suggesting a match.
        \item A Hamming distance of 1 indicates differing indices, suggesting a non-match.
    \end{itemize} 
    
    \item \textbf{Statistical Analysis and Probability Computation}:
    \begin{itemize}
        \item \textbf{\(\mu_{\text{same}}^{obs}\)}: We computed \(\mu_{\text{same}}^{obs}\) by averaging the number of Hamming distances that resulted in a match (Hamming distances of 0) of the intra-individual comparisons, which furnished us with the following probability:

        \[ \mu_{\text{same}}^{obs} \approx 0.22047 = 22.05\%\]
        
        \item \textbf{\(\mu_{\text{diff}}^{obs}\)}: Similarly, \(\mu_{\text{diff}}^{obs}\) was determined by averaging the number of Hamming distances that resulted in a match (Hamming distances of 0) of the inter-individual comparisons, which furnished us with the following probability:

        \[ \mu_{\text{diff}}^{obs} \approx 0.08249 = 8.25\% \]
        \begin{figure}[H]
            \centering
            \begin{subfigure}[b]{0.48\linewidth}
                \centering
                \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/mu_same.png}
                \caption{Count of the number of matches for Same, Aligned Biometric Samples with Single Index preHashing}
                \label{mu_same}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.48\linewidth}
                \centering
                \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/mu_diff.png}
                \caption{Count of the number of matches for Different, Aligned Biometric Samples with Single Index preHashing}
                \label{mu_diff}
            \end{subfigure}
            \caption{Comparison of match counts for Same and Different Aligned Biometric Samples}
        \end{figure}
    \end{itemize}
\end{enumerate}

In our analysis of the dataset at hand featuring biometric data from 20 unique individuals, we have quantified the performance of the preHash function in distinguishing between identical and different biometric subjects. Specifically, when evaluating samples from the same individual, the probability that the preHash function will produce the same index is approximately \(22.05\%\). Conversely, when the function is applied to samples from different individuals, the probability of a match in the indices drops to roughly \(8.25\%\). This indicates that the likelihood of mistakenly identifying different individuals as the same is about three times lower than correctly matching samples from the same individual, demonstrating the preHash function's relative efficacy in biometric differentiation within this specific dataset. Moreover, we can observe that the experimental results that we got are very close to the theoretical values predicted from Table~\ref{tab:approximation_mu_taylor}. This shows that our theoretical framework and the Taylor series approximations accurately model the behavior of \(\mu_{\text{diff}}\) and \(\mu_{\text{same}}\). The minor differences between the theoretical model and the experiments can be explained by the fact that real-world biometric data can include noise and measurement errors that are not accounted for in the theoretical model.


\subsubsection{Experimental Derivation of the Probabilities \(\mu_{same}\) and \(\mu_{diff}\) Excluding the Post-Alignment Step}

We conducted an experimental investigation to determine the impact of omitting the Miura Marching step, which optimizes offsets before comparing images, on the coincidence probabilities of the indices. The Miura Matching step serves as a post-alignment phase in the current pipeline. According to Equation~\ref{eq:mu_leq}, excluding the Miura Matching phase should result in the probabilities of matching indices being less than or equal to the previous values determined for \(\mu_{\text{same}}\) and \(\mu_{\text{diff}}\), respectively.
Following the setup and conductin the experiment as described in the previous section, we obtained the following statistical analysis and probability computations:


\begin{itemize}
    \item \text{\(\mu_{\text{same}}^{obs}\)}: 
    
    \[ \mu_{\text{same}}^{obs} \approx 0.11871 = 11.87\%\]
    
    \item \text{\(\mu_{\text{diff}}^{obs}\)}:
    
    \[ \mu_{\text{diff}}^{obs} \approx 0.05651 = 5.65\% \]

    \begin{figure}[H]
        \centering
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/mu_same_without_postAlignment.png}
            \caption{Count of the number of matches for Same, Non-Aligned Biometric Samples with Single Index preHashing}
            \label{mu_same_without_postAlignement}
        \end{minipage}
        \hfill
        \begin{minipage}[b]{0.48\linewidth}
            \centering
            \includegraphics[width=\linewidth,height=7cm,keepaspectratio]{latex-img/mu_diff_without_postAlignment.png}
            \caption{Count of the number of matches for Different, Non-Aligned Biometric Samples with Single Index preHashing}
            \label{mu_diff_without_postAlignement}
        \end{minipage}
        \caption{Comparison of match counts for Same and Different Non-Aligned Biometric Samples}
    \end{figure}
\end{itemize}

As expected, omitting the Miura Matching step resulted in \(\mu_{\text{same}} = 11.87\%\) and \(\mu_{\text{diff}} = 5.65\%\), which are indeed less than the previously determined values of \(\mu_{\text{same}} = 22.05\%\) and \(\mu_{\text{diff}} = 8.25\%\).

