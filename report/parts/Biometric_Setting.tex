\section{Biometric Setting}
%This section is dedicated to establishing the foundational framework for processing and analyzing biometric data derived from finger vein patterns. We explain how the extracted feature vectors from the pipeline are represented and...

This section provides a comprehensive overview of the mathematical and technical aspects involved in fingervein matching, which is essential for comprehending the subsequent discussions in this paper. Additionally, we will provide insight into the process of obtaining these equations and statements, evaluating their relevance and implications within the context of our research. 


\subsection{Theoretical Foundations and Concepts}
%biometric template = is the reference model obtained from the finger images, represented as a vector
%Biometric capture = actual data obtained from the finger image during enrollement or authentication, represented as a bitstring X of length n

We begin by delving into the representation of the biometric data. Finger images, designated as biometric templates and formated as \(250\)x\(386\), result in n = \(96'500\) pixels per image. To enhance processing efficiency, these templates are converted into vectors, departing from their original 2-dimension image structure. 

In the biometric context, each finger serves as a biometric subject, with a corresponding biometric capture represented as a bitstring \(X\) of length \(n\). This capture encapsulates the specific vein pixel information extracted from the finger image, while the biometric template serves as a reference mode derived from these images. 

Each of the \(n\) bits of the biometric capture is designated as \(X_1\),..., \(X_n\), with \(X_i\) set to \(1\) if the \(i\)-th bit corresponds to a vein and \(0\) otherwise. In the case where \(i\) and \(X\) are randomly chosen, we define the probability of pixel \(X_i\) being a vein 

\begin{equation} \label{eq:p}
    Pr[X_i = 1] = p \approx 3.6 \% 
\end{equation}

In biometric authentication and identification, the uniqueness of each biometric capture is encoded in its bits. The objective revolves around discerning the similarity between two biometric captures to verify or identify two individuals. Therefore, the scoring mechansim plays a crucial role in quantatively determining the similarity between biometric captures. This similarity score holds significance in verifying the identity of an individual (authentication) or identifying potential matches in a database (identification). A higher score indicates a greater ressemblance between captures, while a lower score suggests less similarity. This scoring mechanism is indispensable for ensuring the accuracy and reliability of biometric systems. The score of (\(X\), \(Y\)) is computed as

\begin{equation} \label{eq:score}
    \begin{aligned}
        Score(X, Y) &= \frac{HW(X \land Y)}{HW(X) + HW(Y)}\\
        &= \frac{1}{2}-\frac{1}{2}\frac{d_H(X, Y)}{HW(X) + HW(Y)}
    \end{aligned}
\end{equation}

where \(HW\) denotes the Hamming weight and \(d_H\) the Hamming distance. 

In conjunction with the scoring mechanism, Miura matching emerges as a specialized technique for comparing biometric samples. This method entails determining an optimal offset translation, denoted as \(offset * X\) and \(offset * Y\), between two biometric samples to align their features for comparison, thereby compensating for differences in positioning or orientation. The offsets that maximize the similarity score \(Score(offset_X * X, offset_Y * Y)\) are termed as optimal offsets. Furthermore, we designate \(\bar{X}\) = \(offset_X * X\) and \(\bar{Y}\) = \(offset_Y * Y\) as the two biometric captures after applying the optimal offset translations \(offset * X\) and \(offset * Y\).
Thus Miura matching and the scoring mechanism are interconnected components, where Miura matching facilitates the computation of the score, thereby offering insights into the similarity between biometric captures and enhancing the reliability of matching algorithms. 

The probability of the \(i\)-th pixel of two captures not being the same after applying the optimal offset translations depends on the distribution of (\(X\), \(Y\))

\begin{equation} \label{eq:delta}
    \begin{aligned}
        \delta &= Pr[\bar{X}_i \neq \bar{Y}_i]\\
        &= \frac{1}{n}E(d_h(\bar{X}, \bar{Y}))
    \end{aligned}
\end{equation}

It is crucial to distinguish between the types of distributions associated with the two captures. When both captures originate from the same biometric subject, we refer to it as the $\delta_{same}$. Conversely, if the captures are from different subjects, we label it as $\delta_{diff}$. Additionally, when \(X\) and \(Y\) consist of \(2n\) independent random bits with an expected value of \(p\) and no optimal offset is applied, we classify it as $\delta_{indep}$ In this scenario, \(\delta_{indep}\) = \(2p(1-p)\). These distinctions produce the following figures: 

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.25}\begin{tabular}{|c|c|c|}
        \hline
        $\delta_{same}$ & $\delta_{diff}$ & $\delta_{indep}$\\
        \hline
        $4.8\%$ & $6.1\%$ & $6.9\%$\\
        \hline
    \end{tabular}
\caption{Comparison of Distributions: $\delta_{same}$, $\delta_{diff}$, and $\delta_{indep}$}
\end{table}

Utilizing \hyperref[eq:p]{\(p\)} and \hyperref[eq:delta]{\(\delta\)}, we derive the joint distribution for (\(\bar{X}_j\), \(\bar{Y}_j\)) across \(j\) random instances:

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|c|}
        \hline
        & $\bar{Y}_i = 0$ & $\bar{Y}_i = 1$\\
        \hline
        $\bar{X}_i = 0$ & $1 - p - \frac{\delta}{2}$ & $\frac{\delta}{2}$\\
        \hline
        $\bar{X}_i = 1$ & $\frac{\delta}{2}$ & $p - \frac{\delta}{2}$\\
        \hline
    \end{tabular}
    \caption{Joint Distribution of ($\bar{X}_j$, $\bar{Y}_j$) for Random Instances}
\end{table}



% Start this subsection by introducing the mathematical and theoretical concepts that underpin fuzzy hashing. Discuss the relevance of these concepts in the context of biometric data, focusing on how they enable the creation of reliable and secure hashing mechanisms for inherently noisy data.

%     Key Concepts to Cover:
%         Definition and significance of fuzzy hashing
%         Mathematical principles governing the construction of fuzzy hashes
%         Overview of the biometric setting, including the importance of parameters such as pixel dimensions, vein extraction, and the role of random permutations in hashing

% Subsection 2: Experimental Approach

% In the second subsection, outline the methodology of your experiments designed to test the theoretical underpinnings discussed earlier. Describe the setup, the specific objectives of each experiment, and how these experiments are structured to validate the theoretical models of fuzzy hashing.

%     Key Components to Include:
%         Description of the experimental setup and the data used
%         Explanation of how the experiments are designed to reflect the theoretical aspects of fuzzy hashing
%         Details on the implementation of preHash and postHash functions, and the criteria for their evaluation

% Subsection 3: Verifying Theoretical Predictions

% The final subsection is dedicated to comparing the outcomes of your experiments with the theoretical expectations. This involves analyzing the results, discussing any deviations or confirmations, and what these mean for the validity and reliability of fuzzy hashing in biometric data security.

%     Important Aspects to Discuss:
%         Analysis of experimental results against theoretical predictions
%         Discussion on the accuracy of the fuzzy hashing process, including the matching scores and error rates
%         Implications of the findings for biometric data security and future research directions

% Conclusion of Section 1

% Conclude with a summary of the insights gained from bridging theoretical concepts with empirical evidence. Highlight the importance of this integration for advancing the field of biometric security through fuzzy hashing. Reflect on the potential for future developments and applications stemming from your findings.

\subsection{Equation Derivation and Research Relevance}
In order to derive the probability that a pixel is a vein, \(p\) (\ref{eq:p}), the \(\textit{analyze_single_image}\) algorithm processes the set of images captured by two cameras. The algorithm iterates through \(20\) people, left and right, five trials, processing each combination for each camera. For each combination of person, side, finger, and trial for camera \(i\), the algorithm increments an array, \textit{veins[i]}, which accumulates vein detections for each pixel position across all processed images for that camera. It also increments a counter, \textit{image_count[i]}, keeping track of the number of images processed for each camera. 
After processing all images, the algorithm calculates the probability of each pixel being a vein by dividing the sum of vein detections across both cameras by the sum of the image counts for both cameras (\textit{vein[0]} + \textit{vein[1]})/(\textit{image_count[0]} + \textit{image_count[1]}).


\subsection{Part 3}